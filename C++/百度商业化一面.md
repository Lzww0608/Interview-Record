# 百度商业化一面.md



## 你这个bustubDB数据库的最大数据量是多少？



 

## 两道算法题：螺旋打印矩阵、手动实现LRU缓存

LeetCode 54

```c++
class Solution {
public:
    vector<int> spiralOrder(vector<vector<int>>& matrix) {
        int m = matrix.size(), n = matrix[0].size();
        std::vector<int> ans;

        for (int k = 0; ans.size() < m * n; k++) {
            int i = k, j = k;
            while (ans.size() < m * n && j < n - k) {
                ans.push_back(matrix[i][j]);
                j++;
            }
            j--;
            i++;
            while (ans.size() < m * n && i < m - k) {
                ans.push_back(matrix[i][j]);
                i++;
            }
            i--;
            j--;
            while (ans.size() < m * n && j >= k) {
                ans.push_back(matrix[i][j]);
                j--;
            }
            j++;
            i--;
            while (ans.size() < m * n && i > k) {
                ans.push_back(matrix[i][j]);
                i--;
            }
        }

        return ans;
    }
};
```

LeetCode 146

```c++
class Node {
public:
    int key, val;
    Node *prev, *next;
    Node(int k = 0, int v = 0): key(k), val(v) {}
};
class LRUCache {
    Node* dummy;
    std::unordered_map<int, Node*> keyToNode;
    int capacity;

    void remove(Node *x) {
        if (x != nullptr) {
            x->prev->next = x->next;
            x->next->prev = x->prev;
        }
    }

    void pushToFront(Node *x) {
        x->prev = dummy;
        x->next = dummy->next;
        x->prev->next = x;
        x->next->prev = x;
    }

    Node* getNode(int key) {
        auto it = keyToNode.find(key);
        if (it == keyToNode.end()) {
            return nullptr;
        }

        auto node = it->second;
        remove(node);
        pushToFront(node);
        return node;
    }

public:
    LRUCache(int capacity) {
        this->capacity = capacity;
        dummy = new Node();
        dummy->prev = dummy;
        dummy->next = dummy;
    }
    
    int get(int key) {
        auto node = getNode(key);
        if (node == nullptr) {
            return -1;
        }
        return node->val;
    }
    
    void put(int key, int value) {
        auto node = getNode(key);
        if (node != nullptr) {
            node->val = value;
            return;
        }

        node = new Node(key, value);
        if (keyToNode.size() == this->capacity) {
            auto last = dummy->prev;
            keyToNode.erase(last->key);
            remove(last);
            delete last;
        }

        keyToNode[key] = node;
        pushToFront(node);
        return;
    }
};

/**
 * Your LRUCache object will be instantiated and called as such:
 * LRUCache* obj = new LRUCache(capacity);
 * int param_1 = obj->get(key);
 * obj->put(key,value);
 */
```







## unordered_map与map的区别（底层实现，时间复杂度）

![1725245454010](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\1725245454010.png)

### **1. 底层结构不同  map：红黑树  unordered_map: 哈希表**

map的底层使用的是红黑树，每个节点都需要额外的保存父节点，孩子节点和红/黑性质，导致占用空间颇大。

##### 除非是对顺序有特殊要求的场景，不然我们一般不去选择map。

unordered_map的缺点：

- 哈希表的建立会比较麻烦一些，因为要解决哈希冲突。
- 哈希表要处理扩容的问题，扩容会导致时间复杂读上的抖动



### **2.由于底层结构决定使用场景：**

期望有序则用map，期望效率高并且对顺序没有要求则用unordered_map

### **3.查询效率：**

map低: O(logN)，unordered_map: O(1)

### **4.是否扩容：**

map不需要扩容，unordered_map需要进行扩容 扩容的目的：降低哈希冲突

### **5.空间利用率：**

map高，unordered_map低 : 哈希槽并不都能放满

### **6.插入方式：**

map：以红黑树方式插入 unordered_map：按照开散列方式插入，散列插入必然引起冲突问题，需要解决哈希冲突

### **7.迭代器操作：**

map按红黑树中序遍历进行操作 双向++/--     unordered_map按逐个桶往后遍历 单向++

### **8.是否关于key有序：**

map有序(为了有序必然引起要自己实现compare函数)，unordered_map无序

### **9.map是C++98，unordered_mapC++11**







## 你了解的排序算法，快速排序的时间复杂度，快排时间复杂度是稳定的吗？

快速排序的平均和最好时间复杂度是O(nlogn)，最差是O(n^2)。时间复杂度不稳定，排序也是不稳定的。

```cpp
// quickSortIterative
#include <iostream>
#include <stack>
#include <vector>
#include <random>
#include <utility>


std::pair<int, int> partition(std::vector<int> &arr, int l, int r) {
	int pivot = arr[l + rand() % (r - l + 1)];
	int i = l, j = l, k = r + 1;

	while (i < k) {
		if (arr[i] > pivot) {
			std::swap(arr[i], arr[--k]);
		}
		else if (arr[i] < pivot) {
			std::swap(arr[i++], arr[j++]);
		}
		else {
			i++;
		}
	}

	return {j, k-1};
}

void quickSortIterative(std::vector<int> &arr, int l, int r) {
	std::stack<int> st;

	st.push(l);
	st.push(r);

	while (!st.empty()) {
		r = st.top();
		st.pop();
		l = st.top();
		st.pop();

		auto [p, q] = partition(arr, l, r);
		if (p - 1 > l) {
			st.push(l);
			st.push(p-1);
		}

		if (q + 1 < r) {
			st.push(q + 1);
			st.push(r);
		}
	}
}

// 打印数组
void printArray(std::vector<int>& arr, int size) {
	for (int i = 0; i < size; i++) {
		std::cout << arr[i] << " \n"[i==size-1];
	}
}

int main() {
	std::vector<int> arr = { 10, 7, 8, 9, 1, 5 };
	int n = arr.size();
	quickSortIterative(arr, 0, n - 1);
	std::cout << "排序后的数组: ";
	printArray(arr, n);
	return 0;
}
```





## 说说引用和指针的区别

### 1. 定义方式

- **引用**：引用是一个变量的别名，它在定义时必须被初始化，而且不能更改引用的对象。例如：`int &ref = var;`。
- **指针**：指针是一个变量，它存储另一个变量的地址。指针可以在定义后指向不同的对象，可以为空（nullptr），例如：`int *ptr = &var;`。

### 2. 初始化和绑定

- **引用**：引用必须在声明时进行初始化，且一旦绑定到某个变量，无法更改绑定的对象。引用必须始终指向有效的对象。
- **指针**：指针可以在声明时不初始化，也可以之后更改指向的对象。指针可以被设为nullptr（空指针）或者指向不同的变量。

### 3. 内存管理

- **引用**：引用本身没有占用额外的内存空间，它是被引用对象的别名。
- **指针**：指针本身占用内存，因为它存储的是一个地址（通常是4或8个字节，取决于系统架构）。

### 4. 语法操作

- **引用**：使用引用时不需要解引用操作符。引用变量的使用方式与原变量完全一致。
- **指针**：使用指针时需要使用解引用操作符（*）来访问指针指向的值。

### 5. 安全性

- **引用**：引用通常被认为比指针更安全，因为它们始终指向有效的对象，且不能是nullptr或被更改指向。
- **指针**：指针使用起来更加灵活，但容易出现空指针、野指针（指向已释放的内存）、悬空指针（指向已经被销毁的对象）等问题，容易导致程序崩溃或内存泄漏。

### 6. 引用的使用场景

只看两者区别的话，我们发现引用可以完成的任务都可以使用指针完成，并且在使用引用时限制条件更多，那么C++为什么要引入“引用”呢？

限制条件多不一定是缺点，C++的引用在减少了程序员自由度的同时提升了内存操作的安全性和语义的优美性。比如引用强制要求必须初始化，可以让我们在使用引用的时候不用再去判断引用是否为空，让代码更加简洁优美，避免了指针满天飞的情形。除了这种场景之外引用还用于如下两个场景：

1. 引用型参数

   一般我们使用const reference参数作为只读形参，这种情况下既可以避免参数拷贝还可以获得与传值参数一样的调用方式。

   ```cpp
   void test(const vector<int> &data)
   {
       //...
   }
   int main()
   {
     	vector<int> data{1,2,3,4,5,6,7,8};
       test(data);
   }
   ```

2. 引用型返回值

   C++提供了重载运算符的功能，我们在重载某些操作符的时候，使用引用型返回值可以获得跟该操作符原来语法相同的调用方式，保持了操作符语义的一致性。一个例子就是operator []操作符，这个操作符一般需要返回一个引用对象，才能正确的被修改。

   ```cpp
   vector<int> v(10);
   v[5] = 10;    //[]操作符返回引用，然后vector对应元素才能被修改
                 //如果[]操作符不返回引用而是指针的话，赋值语句则需要这样写
   *v[5] = 10;   //这种书写方式，完全不符合我们对[]调用的认知，容易产生误解
   ```

### 7. 指针与引用的性能差距

指针与引用之间有没有性能差距呢？这种问题就需要进入汇编层面去看一下。我们先写一个test1函数，参数传递使用指针：

```cpp
void test1(int* p)
{
    *p = 3;    //此处应该首先判断p是否为空，为了测试的需要，此处我们没加。
    return;
}
```

该代码段对应的汇编代码如下：

```assembly
pushq	%rbp
movq	%rsp, %rbp
movq	%rdi, -8(%rbp)
movq	-8(%rbp), %rax
movl	$3, (%rax)
nop
popq	%rbp
ret
```

上述代码1、2行是参数调用保存现场操作；第3行是参数传递，函数调用第一个参数一般放在rdi寄存器，此行代码把rdi寄存器值（指针p的值）写入栈中；第4行是把栈中p的值写入rax寄存器；第5行是把立即数3写入到**rax寄存器值所指向的内存**中，此处要注意(%rax)两边的括号，这个括号并并不是可有可无的，(%rax)和%rax完全是两种意义，(%rax)代表rax寄存器中值所代表地址部分的内存，即相当于C++代码中的*p，而%rax代表rax寄存器，相当于C++代码中的p值，所以汇编这里使用了(%rax)而不是%rax。

我们再写出参数传递使用引用的C++代码段test2：

```cpp
void test2(int& r)
{
    r = 3;    //赋值前无需判断reference是否为空
    return;
}
```

这段代码对应的汇编代码如下：

```assembly
pushq	%rbp
movq	%rsp, %rbp
movq	%rdi, -8(%rbp)
movq	-8(%rbp), %rax
movl	$3, (%rax)
nop
popq	%rbp
ret
```

我们发现test2对应的汇编代码和test1对应的汇编代码完全相同，这说明C++编译器在编译程序的时候将指针和引用编译成了完全一样的机器码。所以C++中的引用只是C++对指针操作的一个“语法糖”，在底层实现时C++编译器实现这两种操作的方法完全相同。



## 知道哪些智能指针？shared_ptr和unique_ptr有什么区别？

### 1. C++中的智能指针

- **std::unique_ptr**：独占所有权的智能指针，确保指针在整个生命周期内唯一拥有其所指向的对象。
- **std::shared_ptr**：共享所有权的智能指针，可以有多个指针共享对同一个对象的所有权，使用引用计数来管理对象的生命周期。
- **std::weak_ptr**：弱引用智能指针，不增加引用计数，用于解决`std::shared_ptr`的循环引用问题。
- **std::auto_ptr**（C++11已弃用）：早期的独占所有权智能指针，已被`std::unique_ptr`替代，不推荐使用。

### 2. `shared_ptr` 和 `unique_ptr` 的区别

#### **std::unique_ptr**

- **所有权**：`std::unique_ptr` 独占其所管理对象的所有权，不允许多个指针指向同一个对象。所有权只能通过移动（move）转移，不能复制。
- **复制与移动**：不允许拷贝（复制构造或复制赋值）。可以通过`std::move`将所有权从一个`unique_ptr`转移到另一个。
- **用途**：用于确保对象有且只有一个所有者，典型应用包括动态分配的单一资源的管理，如工厂函数的返回值。
- **性能**：由于不需要引用计数的维护，因此`unique_ptr`的性能通常优于`shared_ptr`。

```CPP
#include <memory>

std::unique_ptr<int> p1(new int(10)); // 创建一个unique_ptr
std::unique_ptr<int> p2 = std::move(p1); // p1的所有权转移给p2
```

#### **std::shared_ptr**

- **所有权**：`std::shared_ptr` 允许多个指针共享对同一个对象的所有权，通过引用计数来跟踪对象的生命周期。当引用计数变为0时，对象被销毁。
- **复制与移动**：可以拷贝，拷贝会增加引用计数。移动会转移所有权而不增加计数。
- **用途**：用于多个所有者需要共享对同一个对象的访问的场景，如在多个类或函数间共享资源时。
- **循环引用问题**：当两个`shared_ptr`相互引用时会造成循环引用，导致对象无法释放。此问题可以通过`std::weak_ptr`解决。

```CPP
#include <memory>

std::shared_ptr<int> p1 = std::make_shared<int>(10); // 创建一个shared_ptr
std::shared_ptr<int> p2 = p1; // p2和p1共享同一个对象，引用计数为2
```

### 3. `std::weak_ptr`

- **特性**：`std::weak_ptr` 是一种不拥有对象的智能指针，它不会增加引用计数，常用于打破`std::shared_ptr`的循环引用。
- **用法**：通常与`std::shared_ptr`搭配使用，获取`weak_ptr`所指对象时需要通过`lock()`方法将其转为`std::shared_ptr`。

### 总结

- **unique_ptr**：独占所有权，不能共享，适合单一拥有者的场景。
- **shared_ptr**：共享所有权，通过引用计数管理生命周期，适合多个拥有者的场景。
- **weak_ptr**：弱引用，不增加引用计数，用于防止`shared_ptr`的循环引用问题。





## 进程和线程有什么区别？

`Linux`内核中都表示为`task_struct`

#### 本质区别：

进程是资源分配的基本单位，线程是CPU调度的基本单位

1. **定义和结构**：
   - **进程**：进程是程序的一个实例，它在其自己的地址空间中运行。每个进程至少包含一个线程（主线程），并拥有自己的虚拟内存、系统资源和独立的执行环境。进程之间互相隔离，一个进程的崩溃通常不会影响到其他进程。
   - **线程**：线程是进程中的执行单元，也被称为轻量级进程。一个进程可以包含多个线程，它们共享父进程的地址空间和资源，如内存和文件句柄等。线程主要用于实现任务的并行执行。
2. **资源共享**：
   - **进程**：进程之间不共享内存或资源，除非通过进程间通信（IPC）机制显式共享，如套接字、信号量、共享内存等。
   - **线程**：同一进程内的线程共享内存和资源，这使得线程间的数据交换和通信更为容易和快速，但也需要注意同步和互斥问题，以避免竞态条件和死锁。
3. **开销和性能**：
   - **进程**：创建和管理进程的开销相对较大，因为每个进程需要独立的地址空间和系统资源。进程切换涉及更多的时间和资源，如保存和加载不同进程的上下文。
   - **线程**：线程的创建和切换开销较小，因为它们共享相同的环境和资源。线程间的切换只需要较少的资源重新配置。
4. **通信方式**：
   - **进程**：进程间通信需要特定的机制，如管道、消息队列、共享内存等，这些机制通常涉及更复杂的设置和管理。
   - **线程**：由于线程共享同一内存空间，它们可以直接通过读写同一内存区域来进行通信，但这要求程序正确地处理同步问题，以防数据不一致。
5. **应用场景**：
   - **进程**：适用于需要独立运行和资源管理的应用，例如在需要隔离的环境中运行不同的服务时。
   - **线程**：适用于需要高效执行并行任务的情况，尤其是在计算密集型应用中，如服务器端程序、复杂算法的实现等。







## 进程间有哪些通信方式？

### 1. **管道（Pipes）**

- **无名管道（Anonymous Pipe）**：通常用于具有亲缘关系的进程之间（如父子进程），数据是单向的（从写端到读端）。
- **命名管道（Named Pipe/FIFO）**：支持无亲缘关系的进程之间的通信，数据也是单向的，但可以通过命名来标识和访问管道。

### 2. **信号（Signals）**

- 信号是用于通知进程发生了某个事件的机制，可以用来控制进程或通知进程执行某些操作（如SIGKILL, SIGTERM）。
- 信号量（Semaphore）是基于信号机制的一种用于进程同步的方式，但信号量主要用于同步而不是数据传输。

### 3. **消息队列（Message Queues）**

- 消息队列允许进程以消息的形式进行通信，消息可以带有类型，并可以按优先级处理。
- 消息队列的优势在于数据有结构化，可以按需读取，而不必按照顺序。

### 4. **共享内存（Shared Memory）**

- 共享内存允许多个进程共享一个内存段，是最快的进程间通信方式之一，因为数据不需要在进程之间复制。
- 需要同步机制（如信号量）来控制对共享内存的访问。

### 5. **信号量（Semaphores）**

- 信号量用于多个进程之间的同步控制，而不是直接的数据传输。
- 它用于解决共享资源的并发访问问题，例如在共享内存通信中控制对共享内存的访问。

### 6. **套接字（Sockets）**

- 套接字用于网络通信，也可用于同一台主机上不同进程之间的通信。
- 套接字支持跨网络、不同主机之间的进程通信，支持 TCP 和 UDP 等多种协议。

### 7. **内存映射文件（Memory-Mapped Files）**

- 通过将文件映射到进程的地址空间中，不同进程可以通过映射到相同的文件来共享数据。
- 适用于需要共享大数据块的场景，支持文件持久化。

### 8. **管道和消息传递模型（Message Passing Model）**

- **POSIX 消息队列**：一种消息传递方式，与 SysV 消息队列类似，但提供了更多的特性，如非阻塞 I/O 等。
- **ZeroMQ 等库**：提供高级消息传递接口，支持多种模式（请求-回复，发布-订阅等）。

### 9. **信号量和互斥锁（Mutexes）**

- 虽然主要用于同步而非数据传递，但也可以作为一种辅助通信手段，例如通过标志位传递简单的状态信息。

### 10. **其他高级通信方式**

- **D-Bus**：用于桌面应用的高层次进程间通信，特别在 Linux 系统中常用。
- **gRPC、Thrift 等 RPC 框架**：用于跨进程调用服务，适用于分布式系统。



## 从输入URL到显示网页，中间有哪些过程？

### 1. **URL 解析**

- 浏览器首先解析用户输入的 URL，确定协议（如 HTTP、HTTPS）、主机名（如 www.example.com）、端口（如果指定）、路径、查询参数等。

### 2. **DNS 解析**

- 浏览器需要将域名（如 www.example.com）解析为 IP 地址。
- 浏览器会首先查找本地缓存（浏览器缓存、操作系统缓存）是否已有该域名的解析记录。
- 如果本地没有缓存，浏览器会向本地 DNS 服务器发送查询请求。
- 本地 DNS 服务器逐级向上查询（递归查询）或者直接查询权威 DNS 服务器，最终获取 IP 地址。

### 3. **建立 TCP 连接**

- 使用解析得到的 IP 地址，浏览器与服务器建立 TCP 连接。这个过程使用三次握手协议：
  1. 客户端发送 SYN（同步序列编号）包给服务器。
  2. 服务器响应 SYN-ACK（同步确认）包给客户端。
  3. 客户端发送 ACK（确认）包，连接建立完成。

### 4. **发送 HTTP/HTTPS 请求**

- HTTP 请求：
  - 浏览器向服务器发送 HTTP 请求，请求头包含方法（如 GET、POST）、请求的资源路径、浏览器信息、Cookie 等。
- HTTPS 请求：
  - 如果是 HTTPS，请求之前会先进行 TLS/SSL 握手以加密通信。握手包括协商加密算法、交换密钥、验证证书等步骤。

### 5. **服务器处理请求并响应**

- 服务器收到请求后，处理请求并生成响应。
- 响应包括状态行（如 200 OK）、响应头（内容类型、缓存控制等）、响应体（如 HTML、CSS、JS 文件等）。

### 6. **浏览器接收响应**

- 浏览器接收到服务器的响应后，解析响应内容。
- 如果是重定向（如 301、302 状态码），浏览器会根据 Location 头信息重新发起请求。
- 如果是需要进一步认证（如 401 未授权），浏览器会根据服务器要求进行身份验证。

### 7. **解析 HTML**

- 浏览器开始解析 HTML 文档，构建 DOM 树（Document Object Model）。
- 解析过程中遇到外部资源（如 CSS、JS、图片），浏览器会并行请求这些资源。

### 8. **解析 CSS**

- 浏览器解析 CSS，生成 CSSOM 树（CSS Object Model），用于描述页面样式。

### 9. **解析 JavaScript**

- 浏览器通过 JavaScript 引擎（如 V8）解析和执行 JavaScript 代码。
- JavaScript 可能会修改 DOM 或 CSSOM（如动态添加内容或样式）。

### 10. **构建渲染树**

- 浏览器将 DOM 树和 CSSOM 树合并成渲染树（Render Tree），用于描述哪些节点需要绘制以及如何绘制。
- 渲染树只包含需要显示的节点（如隐藏的元素不会在渲染树中）。

### 11. **布局（Layout）**

- 浏览器根据渲染树计算每个节点在页面中的位置和大小，这个过程称为布局（也叫排版）。

### 12. **绘制（Painting）**

- 布局完成后，浏览器将渲染树中的每个节点转换为屏幕上的像素，将页面绘制出来。

### 13. **显示**

- 最终，浏览器将页面呈现给用户，用户可以看到并与之交互。

### 14. **持续加载和渲染**

- 浏览器会继续加载、渲染后续的资源（如图片、字体等）。
- 在用户与页面交互（如滚动、点击）时，浏览器可能需要重排（Reflow）和重绘（Repaint）页面。



## 你的Raft KV项目中提到多集群负载集群，是如何实现负载均衡的？

- 轮询算法（Round Robin）：

轮询算法是最简单的负载均衡算法之一，它按照请求的顺序依次将每个请求分配到不同的服务器上。当有新的请求到来时，负载均衡器会依次将请求发送到不同的服务器，直到所有的服务器都被轮询过一遍，然后再从头开始。

- 最小连接数算法（Least Connections）：

最小连接数算法会将新的请求分配到当前连接数最少的服务器上，以确保各服务器的负载尽可能均衡。这种算法考虑了服务器的负载情况，优先将请求发送到负载较低的服务器上。

- 最少响应时间算法（Least Response Time）：

最少响应时间算法会将请求发送到响应时间最短的服务器上，以保证响应时间的最小化。这种算法通常需要负载均衡器记录每个服务器的响应时间，并动态调整请求的分配策略。

- 哈希算法（Hashing）：

哈希算法根据请求的某些属性（如客户端IP地址、URL等）计算哈希值，并将请求发送到对应哈希值的服务器上。这种算法能够确保相同请求始终被发送到同一台服务器上，适用于需要保持会话一致性的场景。

- 加权轮询算法（Weighted Round Robin）：

加权轮询算法在轮询算法的基础上引入了权重的概念，不同的服务器具有不同的权重值。根据权重值的不同，负载均衡器会调整请求的分配比例，以实现负载均衡。

- 拓展：hash环也是一种重要的负载均衡算法，也可以提及。